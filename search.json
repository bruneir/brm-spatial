[
  {
    "objectID": "index.html#preliminaries",
    "href": "index.html#preliminaries",
    "title": "Analysing spatial data using R",
    "section": "Preliminaries",
    "text": "Preliminaries\nWelcome to the first Brunei R User Group meetup!\n\n\n\n\n\n\n\n\nThe RUGS mission is to facilitate the person-to-person exchange of knowledge in small group settings on a global scale. ‚ÄîR Consortium\n\n\n\n\n\"R\" |&gt; \n  rug(\"b\", _, \"unei\")\n\n\n\nAbout us\n\nA group of UBD-ians and R enthusiasts\nWe want to create a community of R users in Brunei\nChampion the Open Source cause\n\nMore events to come this year. Stay tuned!\n\nExpectations\n\n\n\n\n\n\nOutcomes\n\n\n\nThis is a hands-on, live-coding, lecture-style ‚Äúworkshop‚Äù. Expect to learn (or at the very least, see me do!)‚Ä¶\n\nWhat spatial data is and why it‚Äôs important.\nWhat statistical analysis can be done with spatial data.\nHow to perform spatial analysis using R.\n\nA basic understanding of R is assumed.\n\n\n\nFor some, maybe it will be a bit fast-paced (sorry in advanced!). All the materials will be available online (see link on the right). Truthfully, I am not expecting you to walk away as an expert in spatial analysis, given the length of this talk. At the very least however I hope you become aware of the various spatial techniques and how they can be achieved in R.\nI‚Äôm very happy to answer questions afterwards! üòÄ\n\n\nGetting started with R\nI‚Äôll just talk about these points briefly:\n\nWhat‚Äôs the difference between R and RStudio?\nQuick run through RStudio‚Äôs features\nSet up a project\nR Scripts vs Notebooks (.qmd or .Rmd)\nExecuting commands in R\n\n\n# Try this out for yourself!\n\nN &lt;- 100\nx &lt;- runif(n = N, min = 0, max = 1)\nhead(x)  # Show the first 6 elements\n\n[1] 0.71230667 0.18796963 0.68010380 0.06699595 0.62248827 0.91475605\n\nmean(x)  # Calculate the mean of this vector\n\n[1] 0.540429\n\nrun_x &lt;- rep(NA, length(x))\nfor (i in seq_along(x)) {\n  run_x[i] &lt;- sum(x[1:i])\n}\nhead(run_x)\n\n[1] 0.7123067 0.9002763 1.5803801 1.6473760 2.2698643 3.1846204\n\n\n\n\nList of packages\nThe power of R comes from its diverse range of user-contributed packages. To install a package in R, we type install.packages(\"package_name\"). As an example, try install the {tidyverse} package.\n\ninstall.packages(\"tidyverse\")\n\nA bunch of things will happen on your screen that makes you look like a legit hacker. (It‚Äôs normal! Unles‚Ä¶ there are some errors in the installation process üòÖ) Once that‚Äôs done, you will want to load the package using library() to start using it. Here‚Äôs a list of packages we will be using today. You‚Äôll need to install all of them before we begin. In RStudio, there will be a prompt (yellow line at the top of the source pane) for you to install all these packages with a single click.\n\n\nCode\nlibrary(kernlab)       # for GPR\nlibrary(ggrepel)       # nice labels in plots\nlibrary(osrm)          # OSM route calculator\nlibrary(spdep)         # Moran's test and more\nlibrary(CARBayes)      # spatial CAR models\nlibrary(mapview)       # interactive plots\nlibrary(scales)        # pretty formats\nlibrary(tidyverse)\ntheme_set(theme_bw())  # set the theme\n\n\nFurthermore, there are packages that are not yet on CRAN, but are available on GitHub. Please install them using the remotes package.\n\nremotes::install_github(\"propertypricebn/bruneimap\")\n\nOf course, don‚Äôt forget to load it. For more information, please check out the package‚Äôs GitHub page.\n\nlibrary(bruneimap)"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Analysing spatial data using R",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\nWhat we‚Äôll learn\n\n\n\n\nImportance of accounting for spatial effects in data.\nUsing the Moran‚Äôs I test to detect spatial autocorrelations.\n\n\n\nConsider the distribution of house prices in Brunei1. I think it is well accepted that the prices of houses in Brunei are not uniformly distributed across the country, but are determined by location. Figure¬†1 (a) shows a faithful representation of house prices in Brunei, and we can clearly see a clustering effect. Many economists, realtors, and urban planners will undoubtedly tell you that there is some kind of ‚Äúneighbouring-effect‚Äù at play here. House closer to each other tend to exhibit more similar properties.\n\nCode\nload(\"data/artificial_hsp.RData\")\n\n# forest_col &lt;- adjustcolor(\"forestgreen\", alpha = 0.9)\nforest_col &lt;- \"grey\"\n\nplot_hsp &lt;- function(x) {\n  left_join(kpg_sf, x, by = join_by(id, kampong, mukim)) |&gt;\n    ggplot() +\n    geom_sf(aes(fill = price, col = \"\")) +\n    scale_fill_viridis_c(\n      labels = scales::dollar, \n      name = \"Predicted price\", \n      alpha = 0.9, \n      na.value = forest_col\n    ) +\n    scale_colour_manual(values = \"grey30\") +              \n    guides(colour = guide_legend(\n      \"No property\\nsales\", \n      override.aes = list(fill = forest_col, colour = forest_col)\n    ))\n}\n\np1 &lt;- plot_hsp(hsp_bn)\np2 &lt;- \n  hsp_bn |&gt; \n  mutate(price = sample(price)) |&gt; \n  plot_hsp()\n\np1\np2\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original distribution\n\n\n\n\n\n\n\n\n\n\n\n(b) Random shuffle\n\n\n\n\n\n\n\nFigure¬†1: Spatial distribution of house prices in Brunei\n\n\n\nWhen we perform statistical modelling and ignore the spatial component, effectively what we are assuming is that the prices of houses are independent of their location. Under this assumption, Figure¬†1 (b) is a valid representation of house prices in Brunei (uniformity of prices). This is a very dangerous thing to do, especially when the spatial effect are non-ignorable. We may get nonsensical results and inferences out of the resulting model.\nOf course, the degree to which this assumption is violated depends on the context and the problem at hand, as well as the scale of the data. For the above-type problem where we have area-level data, one can use the Moran‚Äôs I test of global heterogeneity to test for spatial autocorrelation. It tests\n\n\\(H_0\\): No spatial autocorrelation\n\\(H_1\\): (Positive) Spatial autocorrelation present\n\nTo test this in R, we can use the spdep::moran.test() function from the {spdep} package. There are two key ingredients here, the actual data itself (house prices), and also the neighbourhood relationship between all the areas (spatial structure). The code is as follows.\n\n\nCode\n# Prepare neighbourhood strucuture\nhsp_sf &lt;- left_join(kpg_sf, hsp_bn, by = join_by(id, kampong, mukim))\nnb &lt;- poly2nb(hsp_sf, queen = FALSE)\nWlist &lt;- nb2listw(nb, zero.policy = TRUE, style = \"B\")\n\n# Moran's test\nmoran.test(hsp_sf$price, listw = Wlist, zero.policy = TRUE, na.action = na.omit)\n\n\n\n    Moran I test under randomisation\n\ndata:  hsp_sf$price  \nweights: Wlist \nomitted: 7, 11, 13, 20, 57, 70, 77, 107, 146, 147, 167, 169, 171, 183, 185, 187, 198, 209, 218, 229, 231, 234, 249, 250, 253, 260, 262, 263, 264, 270, 271, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 299, 305, 311, 312, 329, 353, 357, 358, 362, 378, 379, 380, 384, 388, 402, 404, 405, 407, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 429, 431, 433, 438 \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 22.773, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.794275682      -0.002849003       0.001225259 \n\n\n\nTypes of GIS data\n\n\n\n\n\n\nWhat we‚Äôll learn\n\n\n\n\nTypes of GIS data and how these are handled in R.\nDifference between spatial and non-spatial data analysis.\nImportance of geocoding your data for spatial analysis.\n\n\n\nRoughly speaking, there are 4 types of GIS data.\n\nPoints\n\nHaving \\((X, Y)\\) coordinates (latitude, longitude, or projected coordinates, and are ‚Äúzero-dimensional‚Äù.\nE.g. shopping malls, hospitals, outbreaks, etc.\n\nLines\n\nA collection of points that form a path or a boundary. Has length.\nE.g. roads, rivers, pipelines, etc.\n\nPolygons\n\nA closed area made up of line segments or curves.\nE.g. countries, districts, buildings, etc.\n\nRaster\n\nPixelated (or gridded) data where each pixel is associated with a geographical area and some measurement.\nE.g. satellite images, elevation data, etc.\n\n\nThe first three are usually referred to as vector data. GIS data can be stored in various formats such as .shp or .geojson. The handling of GIS data (at least vector type data) is facilitated by the {sf} package (Pebesma and Bivand 2023) which uses the simple features standard.\n\n\n\n\n\n\nNote\n\n\n\nSimple features refers to a formal standard (ISO 19125-1:2004) that describes how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects.\n\n\nIt‚Äôs helpful to think about the shape of this spatial data set. As an example, here‚Äôs a random slice of 10 kampong-level population data for Brunei:\n\n\nCode\nleft_join(\n  kpg_sf, \n  bn_census2021, \n  by = join_by(id, kampong, mukim, district)\n) |&gt;\n  select(\n    kampong, population, geometry\n  ) |&gt;\n  slice_sample(n = 10)\n\n\nSimple feature collection with 10 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 114.2895 ymin: 4.339887 xmax: 115.1305 ymax: 5.020562\nGeodetic CRS:  WGS 84\n# A tibble: 10 √ó 3\n   kampong                          population                          geometry\n   &lt;chr&gt;                                 &lt;dbl&gt;                     &lt;POLYGON [¬∞]&gt;\n 1 Perumahan Negara Rimba Kawasan 2       4151 ((114.9158 4.954169, 114.9151 4.‚Ä¶\n 2 Kg. Tasek Meradun                       312 ((114.8916 4.872824, 114.8916 4.‚Ä¶\n 3 Kg. Ujong Jalan                         104 ((115.0389 4.692171, 115.039 4.6‚Ä¶\n 4 Hutan Simpan                             NA ((114.9283 4.971709, 114.9277 4.‚Ä¶\n 5 Kg. Sibut                               192 ((115.1263 4.625564, 115.1263 4.‚Ä¶\n 6 Pulau Muara Besar                        NA ((115.1302 4.996881, 115.1301 4.‚Ä¶\n 7 Kg. Kuala Tutong                        238 ((114.6168 4.787795, 114.6165 4.‚Ä¶\n 8 Hutan Simpan Kuala Balai                 NA ((114.4216 4.557172, 114.4215 4.‚Ä¶\n 9 Kg. Sungai Buloh                       4351 ((115.0238 4.995128, 115.0238 4.‚Ä¶\n10 Kg. Belais Kecil                         17 ((115.0507 4.667922, 115.0506 4.‚Ä¶\n\n\nSpatial data analysis must have these two components:\n\nThe study variables (in the above example, this is population data).\nGIS data regarding that study variable.\n\nIf we only have 1 without 2, then it really is just a regular data analysis (stating the obvious). Adding the GIS data is a process called ‚Äúgeocoding‚Äù the data points.\n\n\n\n\n\n\nNote\n\n\n\nIn R, geocoding using {tidyverse} can be achieved using the dplyr::left_join() or similar xxx_join() family of functions."
  },
  {
    "objectID": "index.html#multipoint-data",
    "href": "index.html#multipoint-data",
    "title": "Analysing spatial data using R",
    "section": "(MULTI)POINT data",
    "text": "(MULTI)POINT data\n\n\n\n\n\n\nWhat we‚Äôll learn\n\n\n\n\nLoading data sets in R using readr::read_csv().\nIdentifying data types and their implications.\n\n\n\nUse the data from Jaafar and Sukri (2023) on the physicochemical characteristics and texture classification of soil in Bornean tropical heath forests affected by exotic Acacia mangium. There are three datasets provided.\n\nGIS data (WGS84 coordinates) of all study plots.\nSoil physicochemical property data. This contains details of soil physical, chemical, nutrient concentration of the three habits studied.\nSoil texture classification. Provides details on the classification of the soil texture in the habitats studied.\n\nWe will first load the data sets in R.\n\n\nCode\n## Load the data sets\nsoil_gps &lt;- read_csv(\n  \"data/8389823/GPS - Revised.csv\", \n  # IMPORTANT!!! The csv file has latin1 encoding as opposed to UTF-8\n  locale = readr::locale(encoding = \"latin1\")\n)\n  \nsoil_physico &lt;- read_csv(\"data/8389823/Soil physicochemical properties.csv\")\nsoil_texture &lt;- read_csv(\"data/8389823/Soil texture classification.csv\")\n\n\n\nClean up the point data\n\n\n\n\n\n\nWhat we‚Äôll learn\n\n\n\n\nHighlighting the need for cleaning and preprocessing data.\nUsing glimpse() to peek at the data.\nUsing mutate() to change stuff in the data set.\nUsing str() to look at the structure of an R object.\n\n\n\nLet‚Äôs take a look at the point data set.\n\nglimpse(soil_gps)\n\nRows: 18\nColumns: 5\n$ Forest_type  &lt;chr&gt; \"Kerangas\", \"Kerangas\", \"Kerangas\", \"Kerangas\", \"Kerangas‚Ä¶\n$ Habitat_type &lt;chr&gt; \"Intact\", \"Intact\", \"Intact\", \"Intact\", \"Intact\", \"Intact‚Ä¶\n$ Plot_name    &lt;chr&gt; \"KU1\", \"KU2\", \"KU3\", \"KU4\", \"KU5\", \"KU6\", \"KI1\", \"KI2\", \"‚Ä¶\n$ Latitude     &lt;chr&gt; \"4¬∞ 35' 53.40\\\"N\", \"4¬∞ 35' 38.37\\\"N\", \"4¬∞ 35' 53.89\\\"N\", ‚Ä¶\n$ Longitude    &lt;chr&gt; \"114¬∞ 30' 39.09\\\"E\", \"114¬∞ 31' 05.89\\\"E\", \"114¬∞ 30' 38.90‚Ä¶\n\n\nThe first three columns are essentially the identifiers of the plots (forest type, habitat type, and the unique identification code for the study plot). However, the latitude and longitude needs a bit of cleaning up, because it‚Äôs currently in character format. This needs to be in a formal Degree Minute Second DMS class that R can understand. For this we will use the sp::char2dms() function.\nAs an example let‚Äôs take a look at the first latitude.\n\nx &lt;- soil_gps$Latitude[1]\nx\n\n[1] \"4¬∞ 35' 53.40\\\"N\"\n\n# convert it using sp::char2dms() function\nx &lt;- sp::char2dms(x, chd = \"¬∞\")\nx\n\n[1] 4d35'53.4\"N\n\nstr(x)\n\nFormal class 'DMS' [package \"sp\"] with 5 slots\n  ..@ WS : logi FALSE\n  ..@ deg: int 4\n  ..@ min: int 35\n  ..@ sec: num 53.4\n  ..@ NS : logi TRUE\n\n\nThis is a special class that R understands as being a latitude from Earth. To convert it to decimal, we just do as.numeric():\n\nas.numeric(x)\n\n[1] 4.598167\n\n\nNow let‚Äôs do this for all the values in the soil_gps data. We will use the dplyr::mutate() function in a pipeline.\n\n\nCode\nsoil_gps &lt;-\n  soil_gps |&gt;\n  mutate(\n    Latitude = as.numeric(sp::char2dms(Latitude, chd = \"¬∞\")),\n    Longitude = as.numeric(sp::char2dms(Longitude, chd = \"¬∞\"))\n  )\nsoil_gps\n\n\n# A tibble: 18 √ó 5\n   Forest_type Habitat_type Plot_name Latitude Longitude\n   &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1 Kerangas    Intact       KU1           4.60      115.\n 2 Kerangas    Intact       KU2           4.59      115.\n 3 Kerangas    Intact       KU3           4.60      115.\n 4 Kerangas    Intact       KU4           4.63      114.\n 5 Kerangas    Intact       KU5           4.60      115.\n 6 Kerangas    Intact       KU6           4.60      115.\n 7 Kerangas    Invaded      KI1           4.59      115.\n 8 Kerangas    Invaded      KI2           4.59      115.\n 9 Kerangas    Invaded      KI3           4.59      115.\n10 Kerangas    Invaded      KI4           4.59      115.\n11 Kerangas    Invaded      KI5           4.59      115.\n12 Kerangas    Invaded      KI6           4.59      115.\n13 Kerangas    Plantation   AP1           4.59      115.\n14 Kerangas    Plantation   AP2           4.59      115.\n15 Kerangas    Plantation   AP3           4.59      115.\n16 Kerangas    Plantation   AP4           4.59      115.\n17 Kerangas    Plantation   AP5           4.59      115.\n18 Kerangas    Plantation   AP6           4.59      115.\n\n\n\n\nPreliminary plot of the data\n\n\n\n\n\n\nWhat we‚Äôll learn\n\n\n\n\nStructure of a ggplot() (grammar of graphics).\nUsing geom_sf() to plot the GIS data, and adding points using geom_point().\n\n\n\nUsing the data contained in the {bruneimap} package, we can plot the study areas on a map of Brunei. Use either the brn_sf, dis_sf, mkm_sf or kpg_sf data sets.\n\n\nCode\nggplot(brn_sf) +\n  geom_sf() +\n  geom_point(data = soil_gps, aes(Longitude, Latitude)) \n\n\n\n\n\n\n\n\n\nWe can zoom in a bit‚Ä¶ but we have to find out manually the correct bounding box.\n\n\nCode\nggplot(mkm_sf) +\n  geom_sf() +\n  geom_sf(data = dis_sf, fill = NA, col = \"black\", linewidth = 1) +\n  geom_point(data = soil_gps, aes(Longitude, Latitude)) +\n  geom_text_repel(\n    data = soil_gps,\n    aes(Longitude, Latitude, label = Plot_name),\n    box.padding = 0.5,\n    max.overlaps = 30\n  ) +\n  coord_sf(\n    xlim = c(114.4, 114.6),\n    ylim = c(4.5, 4.7)\n  )\n\n\n\n\n\n\n\n\n\n\n\nMerge with the study data\n\n\n\n\n\n\nWhat we‚Äôll learn\n\n\n\n\nUsing left_join() to merge two data sets together.\nUsing geom_jitter() to plot the study variables that are overlapping.\n\n\n\nLet‚Äôs take a look at the data set.\n\nglimpse(soil_physico)\n\nRows: 144\nColumns: 16\n$ Habitat_type              &lt;chr&gt; \"Intact\", \"Intact\", \"Intact\", \"Intact\", \"Int‚Ä¶\n$ Plot_name                 &lt;chr&gt; \"KU1\", \"KU1\", \"KU1\", \"KU1\", \"KU1\", \"KU1\", \"K‚Ä¶\n$ Subplot_name              &lt;chr&gt; \"A\", \"A\", \"B\", \"B\", \"C\", \"C\", \"D\", \"D\", \"A\",‚Ä¶\n$ Soil_depth                &lt;chr&gt; \"0-15\", \"30-50\", \"0-15\", \"30-50\", \"0-15\", \"3‚Ä¶\n$ Nitrogen                  &lt;dbl&gt; 0.617, 0.188, 0.663, 0.200, 0.465, 0.255, 0.‚Ä¶\n$ Phosphorus                &lt;dbl&gt; 0.248, 0.129, 0.259, 0.295, 0.172, 0.145, 0.‚Ä¶\n$ Magnesium                 &lt;dbl&gt; 0.000, 0.045, 0.054, 0.035, 0.079, 0.043, 0.‚Ä¶\n$ Calcium                   &lt;dbl&gt; 0.167, 0.187, 0.148, 0.113, 0.253, 0.229, 0.‚Ä¶\n$ Potassium                 &lt;dbl&gt; 0.059, 0.037, 0.054, 0.022, 0.098, 0.033, 0.‚Ä¶\n$ Exchangable_magnesium     &lt;dbl&gt; 0.009, 0.004, 0.007, 0.005, 0.029, 0.014, 0.‚Ä¶\n$ Exchangable_calcium       &lt;dbl&gt; 0.010, 0.009, 0.008, 0.009, 0.109, 0.041, 0.‚Ä¶\n$ Exchangable_potassium     &lt;dbl&gt; 0.101, 0.085, 0.092, 0.087, 0.101, 0.090, 0.‚Ä¶\n$ Available_phosphorus      &lt;dbl&gt; 0.012, 0.012, 0.013, 0.012, 0.013, 0.014, 0.‚Ä¶\n$ pH                        &lt;dbl&gt; 2.3, 2.7, 2.0, 2.0, 2.6, 2.5, 2.3, 2.1, 1.0,‚Ä¶\n$ Gravimetric_water_content &lt;dbl&gt; 5.911, 3.560, 10.860, 5.082, 6.963, 4.549, 5‚Ä¶\n$ Organic_matter            &lt;dbl&gt; 4.559, 1.399, 4.523, 2.309, 3.131, 2.209, 3.‚Ä¶\n\nglimpse(soil_texture)\n\nRows: 144\nColumns: 8\n$ Habitat_type           &lt;chr&gt; \"Intact\", \"Intact\", \"Intact\", \"Intact\", \"Intact‚Ä¶\n$ Plot_name              &lt;chr&gt; \"KU1\", \"KU1\", \"KU1\", \"KU1\", \"KU2\", \"KU2\", \"KU2\"‚Ä¶\n$ Subplot_name           &lt;chr&gt; \"A\", \"B\", \"C\", \"D\", \"A\", \"B\", \"C\", \"D\", \"A\", \"B‚Ä¶\n$ Soil_depth             &lt;chr&gt; \"0-15\", \"0-15\", \"0-15\", \"0-15\", \"0-15\", \"0-15\",‚Ä¶\n$ Clay                   &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 2.5, 2.5, 2.5, 0.0, 2.‚Ä¶\n$ Silt                   &lt;dbl&gt; 2.5, 0.0, 0.0, 2.5, 0.0, 0.0, 2.5, 2.5, 7.5, 7.‚Ä¶\n$ Sand                   &lt;dbl&gt; 97.5, 100.0, 100.0, 97.5, 100.0, 97.5, 95.0, 95‚Ä¶\n$ Texture_classification &lt;chr&gt; \"Sand\", \"Sand\", \"Sand\", \"Sand\", \"Sand\", \"Sand\",‚Ä¶\n\n\nThe soil_physico and soil_texture data sets contain the same columns, so we might as well merge them together. We will use the dplyr::left_join() function.\n\n\nCode\n# Actually I just want to merge these two together\nsoil_df &lt;- left_join(\n  soil_physico,\n  soil_texture,\n  by = join_by(Habitat_type, Plot_name, Subplot_name, Soil_depth)\n)\nsoil_df\n\n\n# A tibble: 144 √ó 20\n   Habitat_type Plot_name Subplot_name Soil_depth Nitrogen Phosphorus Magnesium\n   &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 Intact       KU1       A            0-15          0.617      0.248     0    \n 2 Intact       KU1       A            30-50         0.188      0.129     0.045\n 3 Intact       KU1       B            0-15          0.663      0.259     0.054\n 4 Intact       KU1       B            30-50         0.2        0.295     0.035\n 5 Intact       KU1       C            0-15          0.465      0.172     0.079\n 6 Intact       KU1       C            30-50         0.255      0.145     0.043\n 7 Intact       KU1       D            0-15          0.285      0.225     0.052\n 8 Intact       KU1       D            30-50         0.057      0.207     0.031\n 9 Intact       KU2       A            0-15          0.37       0.135     0.038\n10 Intact       KU2       A            30-50         0.114      0.168     0.021\n# ‚Ñπ 134 more rows\n# ‚Ñπ 13 more variables: Calcium &lt;dbl&gt;, Potassium &lt;dbl&gt;,\n#   Exchangable_magnesium &lt;dbl&gt;, Exchangable_calcium &lt;dbl&gt;,\n#   Exchangable_potassium &lt;dbl&gt;, Available_phosphorus &lt;dbl&gt;, pH &lt;dbl&gt;,\n#   Gravimetric_water_content &lt;dbl&gt;, Organic_matter &lt;dbl&gt;, Clay &lt;dbl&gt;,\n#   Silt &lt;dbl&gt;, Sand &lt;dbl&gt;, Texture_classification &lt;chr&gt;\n\n\nOnce we‚Äôve done that, the soil_df data set (the study variables) is actually missing the spatial data. We need to geocode it with the soil_gps data set. Again, dplyr::left_join() to the rescue!\n\n\nCode\nsoil_df &lt;- left_join(\n  soil_df, \n  soil_gps,\n  by = join_by(Habitat_type, Plot_name)\n)\n\n\nNow we‚Äôre in a position to plot the study variables on the map. Note that there are only 18 plots in the soil_gps data set, and each plot has repeated measurements. That means when we plot it, it will overlap and look like a single point. So a good thing to do is to jitter the point so it‚Äôs easier to see.\n\n\nCode\nggplot(kpg_sf) +\n  geom_sf(fill = NA) +\n  geom_jitter(\n    data = soil_df, \n    aes(Longitude, Latitude, col = Nitrogen, size = Nitrogen, \n        shape = Habitat_type),\n    width = 0.001, height = 0.001, alpha = 0.7\n  ) +\n  coord_sf(\n    xlim = c(114.46, 114.54),\n    ylim = c(4.58, 4.64)\n  ) +\n  scale_color_viridis_c() +\n  guides(size = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nPredictive models\n\n\n\n\n\n\nWhat we‚Äôll learn\n\n\n\n\nUsing kernlab::gpr() to build a predictive Gaussian process regression (kriging) model.\nImportant to consult an expert in the field before building a model. GIGO!\n\n\n\nAt this stage, we probably want to consult an expert in this field, and ask what variables are important for predicting the nitrogen content in the soil, so that we can build a predictive model. In mathematics, a model is simply a relationship between variables, like so: \\[\ny = f(x_1,x_2,\\dots,x_p) + \\epsilon\n\\] where the \\(x_1,\\dots,x_p\\) are the input variables, and \\(y\\) is the output variable of interest (in this case, nitrogen concentrations). No model can perfectly account for this relationship, so we add a term \\(\\epsilon\\) which represents the error in the model. Ideally this should be as small as possible.\nSeveral models, ranging from classical statistical models to complex machine learning models, are possible:\n\nLinear regression model ‚Äì lm()\nGeneralised additive models (GAM) (Wood 2017) ‚Äì mgcv::gam()\nGaussian process regression (Kriging) ‚Äì kernlab::gausspr()\nRandom forests ‚Äì randomForest::randomForest()\nGeographically weighted regression (GWR) ‚Äì spgwr::gwr()\n\nLet‚Äôs focus on Gaussian process regression (because that‚Äôs one that I know quite well). The idea of GPR is to model the relationship between the input variables and the output variable as a multivariate Gaussian distribution: \\[\nf(x) \\sim \\operatorname{N}\\big(0, K(x,x')\\big)\n\\] where \\(K(x,x')\\) is the covariance function, which measures the similarity between the input variables \\(x\\) and \\(x'\\). The most common covariance function is the squared exponential function: \\[\nK(x,x') = \\exp\\left(-\\frac{1}{2}\\sum_{i=1}^p (x_i - x'_i)^2\\right).\n\\] Let \\(d\\) represent the ‚Äúdistance‚Äù between two points. Then the squared exponential kernel becomes very small when this distance \\(d\\) is large, and very large when \\(d\\) is small. Put another way, since elements of the matrix \\(K\\) represent co-variability, that means two points that are close together will behave similarly, and vice versa. This is very much in line with Tobler‚Äôs first law of geography: ‚ÄúEverything is related to everything else, but near things are more related than distant things‚Äù.\n\n\nCode\nx &lt;- seq(-4, 4, length = 100)\ny &lt;- exp(-x^2)\ntibble(d = x, Kxx = y) |&gt;\n  ggplot(aes(d, Kxx)) +\n  geom_line() \n\n\n\n\n\n\n\n\n\nGiven the assumed behaviour of our function \\(f(x)\\), and what is observed from the data, we can then make predictions about the nitrogen content at unobserved locations. Skipping over a whole lot of mathematics, let‚Äôs just fit this model in R.\n\n\nCode\n# Build a model to predict Nitrogen from all numeric variables. This is\n# definitely not theory based, so just want to show the code.\nsoil_df &lt;-\n  soil_df |&gt;\n  select(where(is.numeric)) \nmod &lt;- gausspr(Nitrogen ~ ., data = soil_df)\n\n\nUsing automatic sigma estimation (sigest) for RBF or laplace kernel \n\n\nCode\nmod\n\n\nGaussian Processes object of class \"gausspr\" \nProblem type: regression \n\nGaussian Radial Basis kernel function. \n Hyperparameter : sigma =  0.0531575869858902 \n\nNumber of training instances learned : 144 \nTrain error : 0.11268641 \n\n\nHaving done that, we now want to prepare a prediction data frame. Essentially, we will rasterise the study area into a predefined grid. For the other variables, we will just set them at their mean values.\n\n\nCode\nxr &lt;- c(114.4, 114.6)\nxx &lt;- seq(xr[1] - 0.01, xr[2] + 0.01, length = 100)\n\nyr &lt;- c(4.5, 4.7)\nyy &lt;- seq(yr[1] - 0.01, yr[2] + 0.01, length = 100)\n\nmean_X &lt;- \n  soil_df |&gt;\n  summarise(across(everything(), mean)) |&gt;\n  select(-Longitude, -Latitude)\n\npred_df &lt;-\n  expand_grid(\n    Longitude = xx,\n    Latitude = yy\n  ) |&gt;\n  bind_cols(mean_X)\n\npred_df$ypred &lt;- predict(mod, newdata = pred_df)\n\n# Additional step: filter points that are outside of the Brunei land area.\npred_sf &lt;- \n  pred_df |&gt;\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) |&gt;\n  st_filter(y = brn_sf[1, ])\n\nggplot() +\n  geom_raster(\n    data = pred_sf,\n    aes(fill = ypred, geometry = geometry),\n    stat = \"sf_coordinates\",\n    alpha = 0.8\n  ) +\n  # geom_raster(data = pred_df, aes(Longitude, Latitude, fill = ypred),\n  #             alpha = 0.8) +\n  geom_sf(data = kpg_sf, fill = NA, inherit.aes = FALSE, col = \"black\") +\n  geom_sf(data = dis_sf, fill = NA, col = \"black\", linewidth = 1) +\n  geom_point(data = soil_gps, aes(Longitude, Latitude, \n                                  shape = Habitat_type)) +\n  geom_text_repel(\n    data = soil_gps,\n    aes(Longitude, Latitude, label = Plot_name),\n    box.padding = 0.5,\n    max.overlaps = 30\n  ) +\n  scale_fill_viridis_c() +\n  scale_colour_viridis_c() +\n  coord_sf(xlim = xr, ylim = yr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nGarbage In Garbage Out! Modelling is as much an art as it is a science. Careful consideration needs to be made as to what is considered a predictor of a variable."
  },
  {
    "objectID": "index.html#line-data-multilinestring",
    "href": "index.html#line-data-multilinestring",
    "title": "Analysing spatial data using R",
    "section": "Line data ((MULTI)LINESTRING)",
    "text": "Line data ((MULTI)LINESTRING)\n\n\n\n\n\n\nWhat we‚Äôll learn\n\n\n\n\nHow to load spatial data sets using sf::read_sf() and editing the CRS using sf::st_transform().\nHow to filter data using dplyr::filter().\nHow to plot line data using ggplot2::geom_sf().\n\n\n\nFor this example, we‚Äôll play with the road network shape file obtained from OpenStreetMaps. The data is in geojson format, so let‚Äôs import that into R.\n\n\nCode\nbrd &lt;- \n  read_sf(\"data/hotosm_brn_roads_lines_geojson/hotosm_brn_roads_lines_geojson.geojson\") |&gt;\n  st_transform(4326)  # SET THE CRS!!! (WGS84)\nglimpse(brd)\n\n\nRows: 25,570\nColumns: 15\n$ name       &lt;chr&gt; \"Simpang 393\", \"Simpang 405\", NA, NA, NA, NA, \"Lebuhraya Tu‚Ä¶\n$ `name:en`  &lt;chr&gt; NA, NA, NA, NA, NA, NA, \"Tutong‚ÄìTelisai Highway\", NA, NA, N‚Ä¶\n$ highway    &lt;chr&gt; \"residential\", \"residential\", \"service\", \"residential\", \"tr‚Ä¶\n$ surface    &lt;chr&gt; NA, NA, NA, NA, NA, \"asphalt\", \"asphalt\", NA, NA, NA, \"asph‚Ä¶\n$ smoothness &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ width      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ lanes      &lt;chr&gt; NA, NA, NA, NA, NA, \"1\", \"2\", NA, NA, NA, \"2\", NA, NA, NA, ‚Ä¶\n$ oneway     &lt;chr&gt; NA, NA, NA, NA, NA, \"yes\", \"yes\", NA, NA, NA, \"no\", \"yes\", ‚Ä¶\n$ bridge     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ layer      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ source     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ `name:ms`  &lt;chr&gt; NA, NA, NA, NA, NA, NA, \"Lebuhraya Tutong‚ÄìTelisai\", NA, NA,‚Ä¶\n$ osm_id     &lt;int&gt; 386886618, 481030903, 512405939, 664532755, 442044892, 6651‚Ä¶\n$ osm_type   &lt;chr&gt; \"ways_line\", \"ways_line\", \"ways_line\", \"ways_line\", \"ways_l‚Ä¶\n$ geometry   &lt;LINESTRING [¬∞]&gt; LINESTRING (114.6236 4.7910..., LINESTRING (114.‚Ä¶\n\n\nThere are 25,570 features in this data set, which may be a bit too much. Let‚Äôs try to focus on the major roads only. This information seems to be contained in the highway column. What‚Äôs in it?\n\n\nCode\ntable(brd$highway)\n\n\n\n     bridleway   construction       cycleway        footway  living_street \n             1             28             73            898             10 \n      motorway  motorway_link           path     pedestrian        primary \n           116            152            140             60            865 \n  primary_link    residential           road      secondary secondary_link \n           332           9023              1            446             79 \n       service          steps       tertiary  tertiary_link          track \n          9876             53            586             59            442 \n         trunk     trunk_link   unclassified \n           460            310           1560 \n\n\nAccording to this wiki, In OpenStreetMap, the major roads of a road network are sorted on an importance scale, from motorway to quaternary road.\n\n\n\nCode\nbrd_mjr &lt;- \n  brd |&gt;\n  filter(highway %in% c(\"motorway\", \"trunk\", \"primary\", \"secondary\")) \nbrd_mjr\n\n\nSimple feature collection with 1887 features and 14 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 114.1906 ymin: 4.516642 xmax: 115.2021 ymax: 5.037115\nGeodetic CRS:  WGS 84\n# A tibble: 1,887 √ó 15\n   name     `name:en` highway surface smoothness width lanes oneway bridge layer\n * &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;\n 1 Lebuhra‚Ä¶ Tutong‚ÄìT‚Ä¶ trunk   asphalt &lt;NA&gt;       &lt;NA&gt;  2     yes    &lt;NA&gt;   &lt;NA&gt; \n 2 Lebuhra‚Ä¶ Tutong‚ÄìT‚Ä¶ trunk   asphalt &lt;NA&gt;       &lt;NA&gt;  3     yes    &lt;NA&gt;   &lt;NA&gt; \n 3 Jalan S‚Ä¶ &lt;NA&gt;      primary asphalt &lt;NA&gt;       &lt;NA&gt;  2     yes    yes    1    \n 4 Jalan S‚Ä¶ &lt;NA&gt;      primary asphalt &lt;NA&gt;       &lt;NA&gt;  2     yes    &lt;NA&gt;   &lt;NA&gt; \n 5 Lebuh R‚Ä¶ Seria‚ÄìBe‚Ä¶ trunk   asphalt &lt;NA&gt;       &lt;NA&gt;  2     yes    &lt;NA&gt;   &lt;NA&gt; \n 6 &lt;NA&gt;     &lt;NA&gt;      trunk   asphalt &lt;NA&gt;       &lt;NA&gt;  2     yes    &lt;NA&gt;   &lt;NA&gt; \n 7 &lt;NA&gt;     &lt;NA&gt;      primary asphalt &lt;NA&gt;       &lt;NA&gt;  1     yes    &lt;NA&gt;   &lt;NA&gt; \n 8 Lebuh R‚Ä¶ Seria‚ÄìBe‚Ä¶ trunk   asphalt &lt;NA&gt;       &lt;NA&gt;  2     yes    yes    1    \n 9 &lt;NA&gt;     &lt;NA&gt;      primary asphalt &lt;NA&gt;       &lt;NA&gt;  2     yes    &lt;NA&gt;   &lt;NA&gt; \n10 Lebuhra‚Ä¶ Telisai‚Äì‚Ä¶ trunk   asphalt &lt;NA&gt;       &lt;NA&gt;  2     yes    &lt;NA&gt;   &lt;NA&gt; \n# ‚Ñπ 1,877 more rows\n# ‚Ñπ 5 more variables: source &lt;chr&gt;, `name:ms` &lt;chr&gt;, osm_id &lt;int&gt;,\n#   osm_type &lt;chr&gt;, geometry &lt;LINESTRING [¬∞]&gt;\n\n\nAnd now a plot of these roads.\n\n\nCode\nggplot() +\n  geom_sf(data = brn_sf) +\n  geom_sf(data = brd_mjr, aes(col = highway), size = 0.5) +\n  # scale_colour_viridis_d(option = \"turbo\")\n  ggsci::scale_colour_npg()\n\n\n\n\n\n\n\n\n\nWith this, I asked ChatGPT what kind of spatial analyses can be done on this data set. It said, when paired with appropriate data, we can do things like:\n\nNetwork Connectivity Analysis\n\nAssess reachability and identify disconnected road network components.\n\nAccessibility and Service Area Analysis\n\nDetermine service areas and catchment areas for essential services.\n\nTraffic Simulation and Management\n\nSimulate traffic flow to identify bottlenecks and suggest optimal routing.\n\nEnvironmental Impact Assessment\n\nEstimate vehicular emissions and model noise pollution from roads.\n\nUrban and Regional Planning\n\nExamine land use compatibility and assess infrastructure development needs.\n\nSafety Analysis\n\nIdentify accident hotspots and assess pedestrian safety.\n\nEconomic Analysis\n\nEvaluate economic accessibility and the impact of road projects.\n\n\nLet‚Äôs pick one of these: Calculate the distance between the centroid of several regions and the major hospital in the Belait district. This analysis guides urban and healthcare planning by pinpointing areas with inadequate access to emergency services, enabling targeted infrastructure and service improvements.\n\nRoad networks in Belait region\n\n\n\n\n\n\nWhat we‚Äôll learn\n\n\n\n\nManipulating GIS data using sf::st_intersection() and the like. Useful for reorganising the spatial structure (without having to do this in QGIS or ArcGIS).\nSampling points from a line data set.\nCalculating distances between points and lines using {osrm} package.\n\n\n\nFirst we ‚Äúcrop‚Äù the road network to the Belait region.\n\n\nCode\nbrd_belait &lt;- st_intersection(\n  brd,\n  filter(dis_sf, name == \"Belait\")\n)\n\nggplot(brd_belait) +\n  geom_sf() +\n  geom_sf(data = filter(dis_sf, name == \"Belait\"), fill = NA)\n\n\n\n\n\n\n\n\n\nIf we were to sample random points from the Belait polygon, we might get non-sensical areas like the extremely rural areas or forest reserves. So the idea is to sample random points from the road network itself. For this, we need a function that will get us a random point on the path itself.\n\n\nCode\nget_random_point &lt;- function(linestring) {\n  coords &lt;- st_coordinates(linestring)\n  samp_coord &lt;- coords[sample(nrow(coords), 1), , drop = FALSE]\n  samp_coord[, 1:3]\n}\nget_random_point(brd_belait$geometry[1])\n\n\n         X          Y         L1 \n114.241941   4.594271   1.000000 \n\n\nOnce we have this function, we need to map() this function onto each of the linestrings in the brd_belait data set. The resulting list of points is too large! So we will just sample 100 points (you can experiment with this number).\n\n\nCode\nrandom_points &lt;-\n  map(brd_belait$geometry, get_random_point) |&gt;\n  bind_rows() |&gt;\n  slice_sample(n = 100)\n\n\nWhat we have now is a data frame of 100 random points on the road network in the Belait district. We will use the {osrm} package to calculate the distance between these points and the Suri Seri Begawan Hospital in Kuala Belait. The output will be three things: 1) The duration (minutes); 2) The distance (km); and 3) a LINESTRING object that represents the path to get to the hospital. Unfortunately the osrmRoute() function is not vectorised, i.e.¬†we have to do it one-by-one for each of the 100 points. Luckily, we can just make a for loop and store the results in a list.\n\n\nCode\nsuriseri &lt;- c(114.198778, 4.583444)\n\nres &lt;- list()\nfor (i in 1:100) {\n  res[[i]] &lt;- osrmRoute(src = random_points[i, 1:2], dst = suriseri, overview = \"full\")\n}\nres &lt;- \n  bind_rows(res) |&gt;\n  as_tibble() |&gt;\n  st_as_sf()\nres\n\n\nSimple feature collection with 100 features and 4 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 114.1891 ymin: 4.26484 xmax: 114.6936 ymax: 4.69931\nGeodetic CRS:  WGS 84\n# A tibble: 100 √ó 5\n   src   dst   duration distance                                        geometry\n   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;                                &lt;LINESTRING [¬∞]&gt;\n 1 1     dst       7.27     4.80 (114.237 4.58995, 114.2368 4.5899, 114.2368 4.‚Ä¶\n 2 1     dst       9.64     7.52 (114.2535 4.58968, 114.2535 4.5897, 114.2535 4‚Ä¶\n 3 1     dst      21.1     16.0  (114.322 4.59995, 114.322 4.60004, 114.322 4.6‚Ä¶\n 4 1     dst      22.1     22.6  (114.3514 4.62299, 114.3508 4.62279, 114.3483 ‚Ä¶\n 5 1     dst       3.93     2.30 (114.2127 4.58419, 114.2128 4.58375, 114.2114 ‚Ä¶\n 6 1     dst       6.39     3.94 (114.2151 4.57494, 114.2148 4.57511, 114.2144 ‚Ä¶\n 7 1     dst      22.1     24.6  (114.368 4.60969, 114.368 4.60969, 114.3683 4.‚Ä¶\n 8 1     dst      18.6     15.1  (114.3207 4.60997, 114.3196 4.6096, 114.3194 4‚Ä¶\n 9 1     dst      11.9      7.87 (114.2609 4.59622, 114.2611 4.59626, 114.2611 ‚Ä¶\n10 1     dst       9.75     7.44 (114.2525 4.58893, 114.2525 4.589, 114.2525 4.‚Ä¶\n# ‚Ñπ 90 more rows\n\n\nSo with all that done, we can now plot the paths taken by the 100 random points to the hospital. The map gives us an indication of which areas are underserved by the hospital, and can guide urban and healthcare planning by pinpointing areas with inadequate access to emergency services, enabling targeted infrastructure and service improvements.\n\n\nCode\nggplot(res) +\n  # geom_point(data = random_points, aes(x = X, y = Y), col = \"red\") +\n  geom_sf(data = filter(kpg_sf, district == \"Belait\"), fill = NA) +\n  geom_sf(aes(col = duration), linewidth = 1.2, alpha = 0.7) +\n  geom_point(x = suriseri[1], y = suriseri[2], col = \"red3\", pch = \"X\", \n             size = 3) +\n  scale_colour_viridis_c() \n\n\n\n\n\n\n\n\n\nImproving the analysis\n\nWeight analysis by populous areas. Outcalls to hospitals can be modelled using a Poisson distribution with the population as the rate parameter.\nUse a more sophisticated routing algorithm that accounts for traffic conditions and road quality (am vs pm, weekends vs weekdays, etc.).\nSimpler to analyse at the kampong or mukim level?"
  },
  {
    "objectID": "index.html#areal-data-multipolygons",
    "href": "index.html#areal-data-multipolygons",
    "title": "Analysing spatial data using R",
    "section": "Areal data ((MULTI)POLYGONS)",
    "text": "Areal data ((MULTI)POLYGONS)\n\n\n\n\n\n\nWhat we‚Äôll learn\n\n\n\n\nRepresent statistical data using colour mapping symbology (choropleth)\nUse ggplot2::geom_label() or ggrepel::geom_label_repel() to add labels to the map\nUsing a binned colour scale, e.g.¬†ggplot2::geom_scale_fill_viridis_b()\n\n\n\nWhen your study data is made up a finite number of non-overlapping areas, then you can represent them as polygons in R. This is the case for the kampong and mukim data in Brunei. As an example, let us look at the population of each kampong in Brunei. This dataset comes from the 2021 Brunei Census data (DEPS 2022)\n\n\nCode\nglimpse(bn_census2021)\n\n\nRows: 365\nColumns: 11\n$ id           &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 14, 15, 16, 17, 18, 19, 2‚Ä¶\n$ kampong      &lt;chr&gt; \"Kg. Biang\", \"Kg. Amo\", \"Kg. Sibut\", \"Kg. Sumbiling Baru\"‚Ä¶\n$ mukim        &lt;chr&gt; \"Mukim Amo\", \"Mukim Amo\", \"Mukim Amo\", \"Mukim Amo\", \"Muki‚Ä¶\n$ district     &lt;chr&gt; \"Temburong\", \"Temburong\", \"Temburong\", \"Temburong\", \"Temb‚Ä¶\n$ population   &lt;dbl&gt; 75, 394, 192, 91, 108, 143, 199, 123, 95, 90, 92, 2427, 4‚Ä¶\n$ pop_male     &lt;dbl&gt; 46, 218, 98, 48, 60, 68, 115, 65, 52, 46, 73, 1219, 252, ‚Ä¶\n$ pop_female   &lt;dbl&gt; 29, 176, 94, 43, 48, 75, 84, 58, 43, 44, 19, 1208, 150, 2‚Ä¶\n$ pop_bruneian &lt;dbl&gt; 37, 280, 174, 55, 57, 64, 114, 88, 63, 35, 37, 1557, 235,‚Ä¶\n$ pop_pr       &lt;dbl&gt; 33, 83, 17, 24, 41, 64, 64, 28, 29, 32, 2, 179, 3, 67, 32‚Ä¶\n$ household    &lt;dbl&gt; 13, 83, 37, 23, 23, 23, 38, 26, 26, 23, 14, 517, 76, 691,‚Ä¶\n$ occ_liv_q    &lt;dbl&gt; 13, 62, 27, 16, 22, 21, 37, 22, 12, 23, 14, 492, 71, 681,‚Ä¶\n\n\nEach row of the data refers to a kampong-level observation. While there are unique identifiers to this (id, kampong, mukim, district), we would still need to geocode this data set so that we can do fun things like plot it on a map. Let‚Äôs use (again) left_join() to do this.\n\n\nCode\nbn_pop_sf &lt;- \n  left_join(\n    kpg_sf, \n    bn_census2021, \n    by = join_by(id, kampong, mukim, district)\n  )\n\n\nGreat. Let‚Äôs take a look at the population column. It would be very interesting to see where most of the 440,704 people of Brunei live!\n\n\nCode\nggplot(bn_pop_sf) +\n  geom_sf(aes(fill = population)) +\n  scale_fill_viridis_c(na.value = NA)\n\n\n\n\n\n\n\n\n\nAs expected, there are ‚Äúhotspots‚Äù of population in the Brunei-Muara district, and to a lesser extent in the Belait district. We can make this graph a bit better by binning the population values. It seems to be dominated by a lot of these low value colours. Let‚Äôs take a look at this further by inspecting a histogram.\n\n\nCode\nggplot(bn_pop_sf) +\n  geom_histogram(aes(population), binwidth = 100)\n\n\n\n\n\n\n\n\n\nSo maybe we can bin the population into 4 categories: &lt; 100, 101-1000, 1001-10000, and 10000+. For this we directly use the scale_fill_viridis_b() and adjust the breaks. Otherwise we would have to cut() the population column and then use scale_fill_manual(). We also added the names of the top 10 most populous kampongs to the map using ggrepel::geom_label_repel().\n\n\nCode\nkpg_labels_sf &lt;-\n  bn_pop_sf |&gt;\n  arrange(desc(population)) |&gt;\n  slice_head(n = 10)\n\nbn_pop_sf |&gt;\n  # filter(population &gt; 50) |&gt;\n  ggplot() +\n  geom_sf(aes(fill = population), col = NA, alpha = 0.8) +\n  geom_sf(data = kpg_sf, fill = NA, col = \"black\") +\n  ggrepel::geom_label_repel(\n    data = kpg_labels_sf,\n    aes(label = kampong, geometry = geometry),\n    stat = \"sf_coordinates\",\n    inherit.aes = FALSE,\n    box.padding = 1,\n    size = 2,\n    max.overlaps = Inf\n  ) +\n  scale_fill_viridis_b(\n    name = \"Population\",\n    na.value = NA,\n    labels = scales::comma,\n    breaks = c(0, 100, 1000, 10000, 20000)\n    # limits = c(0, 12000)\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nCondtional Autoregressive (CAR) models\n\n\n\n\n\n\nWhat we‚Äôll learn\n\n\n\n\nUse spatial autoregressive models (CAR models) to quantify discrepancies in the variable of interest due to the area.\nUse the {CARBayes} package to do this.\nPlot interactive maps using {mapview}.\n\n\n\nOne of the most interesting things about areal data is that you might want to find out how the value of a variable of interest is impacted simply by being in a certain area. A possible model could be \\[\ny_i = \\alpha + f(x_i) + \\phi_i + \\epsilon_i\n\\] where \\(y_i\\) is the value of the variable of interest in area \\(i\\), \\(\\alpha\\) is the intercept, \\(f(x_i)\\) is the covariate predictor, and \\(\\phi_i\\) represents the ‚Äúoffset‚Äù to the average value of \\(y_i\\) due to the area \\(i\\). If you‚Äôre familiar with ANOVA this feels similar to that, except that we impose a certain structure to these \\(\\phi_i\\)s: \\[\n\\phi_i \\mid \\boldsymbol{\\phi}_{-i} \\sim \\operatorname{N} \\left(  \\rho\\sum_{j \\sim i} w_{ij} \\phi_j \\ , \\ \\sigma^2 \\right).\n\\] Here \\(\\rho\\) is a parameter that controls the amount of spatial autocorrelation in the model, and \\(w_{ij}\\) is assumed row-normalised. This means that these random effects \\(\\phi\\) must take into account the structure of the neighbourhoods of the areas when estimating these values. This is the Conditional Autoregressive (CAR) model, and it is a very popular model for areal data.\nLet‚Äôs take a peek at the hsp_bn data that we saw earlier.\n\n\nCode\nglimpse(hsp_bn)\n\n\nRows: 355\nColumns: 8\n$ id        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 182, 183, 184, 185, 187, 189, 191, 192, 19‚Ä¶\n$ kampong   &lt;chr&gt; \"Kg. Biang\", \"Kg. Amo\", \"Kg. Sibut\", \"Kg. Sumbiling Baru\", \"‚Ä¶\n$ mukim     &lt;chr&gt; \"Mukim Amo\", \"Mukim Amo\", \"Mukim Amo\", \"Mukim Amo\", \"Mukim A‚Ä¶\n$ price     &lt;dbl&gt; 111334.17, 90746.26, 229180.29, 134933.77, 113434.67, 107893‚Ä¶\n$ built_up  &lt;dbl&gt; 830, 820, 1300, 1300, 820, 840, 830, 1700, 1700, 810, 2200, ‚Ä¶\n$ land_size &lt;dbl&gt; 0.5150, 0.6140, 0.0421, 0.1800, 0.3100, 2.7200, 0.5800, 0.06‚Ä¶\n$ beds      &lt;dbl&gt; 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 5, 4, 5, 5, 4, 4, 4, 3, 4, 6, ‚Ä¶\n$ baths     &lt;dbl&gt; 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 4, 4, 4, 4, 3, 4, 4, 5, 4, 5, ‚Ä¶\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe hsp_bn data is an artificial data set that I produced for instructive purposes only. It should NOT be used for research purposes.\n\n\nSuppose we are interested in building a linear house price model for Brunei that predicts the price of a house based on the built up size, number of bedrooms, number of bathrooms, and the size of the land it sits on. This will look like this: \\[\n\\texttt{price} = \\alpha + \\beta_1 \\texttt{built\\_up} + \\beta_2 \\texttt{beds} + \\beta_3 \\texttt{baths} + \\beta_4 \\texttt{land\\_size} + \\phi + \\epsilon\n\\] From the earlier plot (Figure¬†1), we saw that there a lot of gaps in the kampong areas, which create ‚Äúislands‚Äù of areas disconnected from each other. Ideally the areas are all interconnected in some way. One way to solve this is to work at the higher mukim level. However, since we have kampong-level data (multiple observations per mukim), we have two approaches:\n\nAggregate the data upwards to the mukim-level. This involves summarising the data for each mukim based on the multiple kampong observations (e.g.¬†using medians, means and modes).\nUtilise the kampong-level data directly, incorporating the multilevel structure into the model.\n\nThe first approach has the downside of reducing the number of observations to the number of mukims, which might not be ideal if you have very few mukims. We only have 37 observed mukims so in terms of sample size strength, this is not great. So, we will use the second approach.\nThe first thing to do is to create the \\(W\\) matrix required for the CAR model, which is deduced from the neighbourhood structure of the mukims. This can be obtained using the spdep::poly2nb() function, as shown below.\n\n\nCode\ndat_sp &lt;-\n  hsp_bn |&gt;\n  group_by(mukim) |&gt;\n  summarise(mukim = first(mukim)) |&gt;\n  left_join(mkm_sf) |&gt;\n  st_as_sf() |&gt;\n  as(\"Spatial\")\nnb_mkm &lt;- poly2nb(dat_sp, row.names = dat_sp$mukim, queen = FALSE)\n\n# Plot\nnb_sf &lt;- nb2lines(nb_mkm, coords = sp::coordinates(dat_sp), as_sf = TRUE)\nnb_sf &lt;- st_set_crs(nb_sf, st_crs(dat_sp))\nggplot() +\n  geom_sf(data = mkm_sf) +\n  geom_sf(data = nb_sf, col = \"red3\")\n\n\n\n\n\n\n\n\n\nThis graph contains 2 subgraphs which are disjoint, because the none of the borders of the mukims in Temburong are found to be touching with any of the other mukims. This is an arbitrary decision, and we can manually change this if we wanted to. Maybe our reasoning is because the bridge is already built, we can consider Mukim Kota Batu and Mukim Bangar to be neighbours.\n\n\nCode\ni &lt;- which(attr(nb_mkm, \"region.id\") == \"Mukim Kota Batu\")\nj &lt;- which(attr(nb_mkm, \"region.id\") == \"Mukim Bangar\")\nnb_mkm[[i]] &lt;- c(nb_mkm[[i]], j)\nnb_mkm[[j]] &lt;- c(nb_mkm[[j]], i)\nnb_mkm\n\n\nNeighbour list object:\nNumber of regions: 37 \nNumber of nonzero links: 152 \nPercentage nonzero weights: 11.10299 \nAverage number of links: 4.108108 \n\n\nThe CAR model requires the \\(W\\) matrix, which is then obtained using the spdep::nb2mat() function. To estimate this model in R, we use the {CARBayes} package. This is done as follows:\n\n\nCode\nmod1 &lt;- CARBayes::S.CARmultilevel(\n  formula = I(price/1000) ~ I(built_up/1000) + beds + baths + land_size,\n  data = hsp_bn,\n  family = \"gaussian\",\n  burnin = 1000,\n  n.sample = 2000,\n  ind.area = match(hsp_bn$mukim, dat_sp$mukim),\n  W = nb2mat(nb_mkm, style = \"B\")\n)\n\n\n\n\nCode\nmod1\n\n\n\n#################\n#### Model fitted\n#################\nLikelihood model - Gaussian (identity link function) \nRandom effects model - Multilevel Leroux CAR\nRegression equation - I(price/1000) ~ I(built_up/1000) + beds + baths + land_size\n\n\n#################\n#### MCMC details\n#################\nTotal number of post burnin and thinned MCMC samples generated - 1000\nNumber of MCMC chains used - 1\nLength of the burnin period used for each chain - 1000\nAmount of thinning used - 1\n\n############\n#### Results\n############\nPosterior quantities and DIC\n\n                      Mean      2.5%     97.5% n.effective Geweke.diag\n(Intercept)       229.6929  212.2468  246.5570       572.6        -1.5\nI(built_up/1000)    9.2163    3.4713   15.1749       617.1         0.3\nbeds                6.0229    1.2554   10.6222       729.0         1.7\nbaths              -1.3460   -5.7400    2.9601       488.3        -0.2\nland_size          -9.4081  -15.9998   -3.0229       571.4         0.2\nnu2               946.8071  806.9922 1115.4532       652.9         1.1\ntau2             5361.9306 3211.7394 8569.8054       751.2        -1.1\nrho                 0.7227    0.4181    0.9696        19.4         0.1\n\nDIC =  3480.954       p.d =  39.92762       LMPL =  -1743.63 \n\n\nA note on scaling of the variables. We divide the price and built_up by 1000. This has the effect of rescaling the price to thousands of Brunei Dollars, and the effect of built_up is then measured per 1000 sqft. This is just to make the output more interpretable (and look nice), which we can do now.\nLet‚Äôs take a look at the mukim-level estimates of the \\(\\phi\\) values.\n\n\nCode\nphi &lt;- apply(mod1$samples$phi, 2, mean)\nnames(phi) &lt;- dat_sp$mukim\nhead(phi)\n\n\n      Mukim Amo    Mukim Bangar Mukim Batu Apoi Mukim Berakas A Mukim Berakas B \n      -86.30471        25.75645       -65.00173        42.76717        17.00908 \n    Mukim Bokok \n      -56.86118 \n\n\nAnd if we wanted to visualise this interactively, we can us the {mapview} package for this.\n\n\nCode\nhsp_mapview &lt;-\n  left_join(mkm_sf, tibble(\n    mukim = names(phi),\n    phi = round(phi * 1000, -3)\n  ))\nmapview(\n  hsp_mapview,\n  zcol = \"phi\",\n  layer.name = \"Spatial effects (BND)\",\n  label = \"mukim\",\n  alpha.regions = 0.9\n)\n\n\n\n\n\n\nNext steps\n\nUsually want to compare against a model with no spatial effects, using e.g.¬†CARBayes::S.glm(), and compare model fit (DIC, WAIC, predictive marginal log-likelihood). This gives (hopefully) justification as to why a spatial model is needed.\nModel spatial effects at the kampong level perhaps?\nAre the variables sufficient for the model? Maybe there are other interaction effects not sufficiently accounted for (e.g.¬†house type? age/condition of property? neighbourhood characteristics? )"
  },
  {
    "objectID": "index.html#summary",
    "href": "index.html#summary",
    "title": "Analysing spatial data using R",
    "section": "Summary",
    "text": "Summary\n\nSpatial data is data that has a spatial component to it, and can be represented as points, lines, polygons, or grids.\nWhen your study variables are spatially dependent, or spatially heterogeneous, you must account for this in your analyses.\nDepending on the type of spatial data you, you can use different types of spatial models.\n\n\nFor modelling point data, you can use something as simple as linear regression (with X,Y coordinates as input) or more complex models like kriging.\nFor line data, the interest is usually in paths and distances.\nFor areal data, you can use spatial autoregressive models (CAR or SAR), or spatially varying coefficient models (e.g.¬†GWR).\n\n\nR has a number of packages that can help you with spatial data analysis!\n\n\n\n\n\n\nFigure¬†1¬†(a): Original distribution\nFigure¬†1¬†(b): Random shuffle"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Analysing spatial data using R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nArtificial data generated for the purposes of this tutorial. Not to be used for research purposes!‚Ü©Ô∏é"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Analysing spatial data using R",
    "section": "",
    "text": "Setup instructions\nPrior to attending the event, please ensure you have done the following.\n\nInstall R and RStudio from https://posit.co/download/rstudio-desktop/.\n\nNote: These are two different things, so please ensure you have installed both of them. Tak a look also at https://bruneir.github.io/posts/welcome/\nInstall the following R packages by running the following code in the RStudio console:\n\ninstall.packages(c(\n  \"tidyverse\", \n  \"sf\", \n  \"kernlab\", \n  \"ggrepel\", \n  \"osrm\", \n  \"remotes\",\n  \"CARBayes\",\n  \"mapview\",\n  \"scales\",\n  \"ggsci\"\n))\n\nInstall the {bruneimap} package by running the following code in the RStudio console.\n\nremotes::install_github(\"propertypricebn/bruneimap\")\n\nDownload the material for this workshop by cloning the GitHub repo or downloading the zip file. Unzip the file (or clone the repo) onto a convenient location on your computer.\n\nClick on brm-spatial.Rproj to open the project in RStudio.\n\n\n\nReady to rock and roll! üé∏\nAddendum\nDepending on the OS you are using, you may need to install additional developer tools.\n\nFor Windows users, you may need to install Rtools from https://cran.r-project.org/bin/windows/Rtools/.\nFor Mac users, you may need to install Xcode from the App Store (or see here https://mac.r-project.org/tools/).\nLinux users should be fine, as the necessary compiler tools are usually installed by default.\n\n\n\n\n\n\nReferences\n\nBivand, Roger S, Edzer J Pebesma, Virgilio G√≥mez-Rubio, and Edzer Jan Pebesma. 2008. Applied Spatial Data Analysis with R. Vol. 747248717. Springer.\n\n\nDEPS. 2022. ‚ÄúThe Population and Housing Census Report (BPP) 2021: Demographic, Household and Housing Characteristics.‚Äù Department of Economic Planning and Statistics, Ministry of Finance and Economy, Brunei Darussalam.\n\n\nJaafar, Salwana Md, and Rahayu Sukmaria Sukri. 2023. ‚ÄúData on the Physicochemical Characteristics and Texture Classification of Soil in Bornean Tropical Heath Forests Affected by Exotic Acacia Mangium.‚Äù Data in Brief 51 (December). https://doi.org/10.1016/j.dib.2023.109670.\n\n\nJamil, Haziq. 2024+. ‚ÄúA Spatio-Temporal Analysis of Property Prices in Brunei Darussalam.‚Äù 2024+. https://haziqj.ml/house-price/papers/jamil2024spatio/.\n\n\nMoraga, Paula. 2019. Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny. CRC Press.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With Applications in R. 1st ed. New York: Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nWood, Simon N. 2017. Generalized Additive Models: An Introduction with R, Second Edition. 2nd ed. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9781315370279."
  }
]